Change the protocol so everytime an ACK is missed, the fault counter is incremented.
If the fault counter reaches a certain critical value, then flush away the LO (or just don't service it)
When we have an unused TX slot, go back to normal

--------------------------------------------------------------------------------

Issues in case of result problems:

Is the timer tracking properly between the global and local transmissions

How to know if the transmissions are successful if we can't see the ACKs?
can we tag it onto other packets etc? as long as there is something to send... or send a monitoring
packet to destination 99 otherwise

Data would contain the recent numbers of packets seen successfully

Can we write to the node internal memory? If so we could log the number of packets seen successfully there?

--------------------------------------------------------------------------------

For when the results are working:

Set up randomisation of the C value in the TXFlow component
Or randomise it on packet creation in the TXFlow component

Check the sequence numbers and burst numbers for the new packets
Test two node communications, especially node 1
Check the timing changes and interference with a single interferer node for faults
Check slot shortening

If we receive a packet with the fault structure e.g. 0x23 0x69
then set activeFault to true
activeFault is cleared on when the fault ends - set a timer to do it
Have the fault packet contain the number of slots 

If there is a fault active, then all nodes act as though they didn't
receive their ACKs, i.e. they refuse to dequeue

One bug to fix: sequence numbers missed out e.g. with flow 6 alone, counting up too many sequence numbers

Ensure the criticalities are set for the various priorities
Why was that "flow 0" chosen for dequeue? (print it out and examine the packet?)
What are the additional packets seen on the global listener? (TimeSync layer?)

Check the periods against those shown for node 2 in the analysis in Alan's paper

--------------------------------------------------------------------------------
Alan questions:

Say I inject with the generator a fault of length 2 tables. If there
is one random failure due to interference, or time sync error just
before and one after, then the fault has covered 4 tables. The actual
fault the nodes see is worse than the worst case we are planning for,
so if we exceed a deadline, we will not know if it is due to protocol
misbehaviour or a fault worse than expected. So maybe I have to record
the maximum missed ACK counter over the course of the evaluation.

Is there any analysis that would determine what the maximum buffer
size we need is?

Also, what actually happens when a destination node is totally dead?
To the protocol, this looks like a fault of infinite duration, which
is beyond what we analysed for. So in this scenario, we would expect
the queue to fill up for messages of that priority.

Note that this frame may be part of a longer message that was split
e.g. into 3 frames. In this case, there's really no point in
discarding a message and ending up with partial data?

--------------------------------------------------------------------------------

If I need more speed for 10ms slots, inline key functions? TinyOS
should do it automatically though

What is the queue actually doing? It is a FIFO queue, isn't it?

TXFlowP - are they injecting too frequently? (seems OK now)

Get flow 11 working - node 4 needs additional slots?

If there is a problem with sync for node 11, can we merge the sync
packet with slot 0's data

--------------------------------------------------------------------------------
Log the reception status of the last packets

Only start the simulation 

Create a flow 1 status record in the buffer - 3 bytes per flow, one
for the ID, the other two for the sequence number. This allows the
reception status to be checked, by verifying that they are seen by
another node.

What to verify - under F=1 fault conditions, all HIs and LOs are
registered successfully delivered by their deadlines.

Under F=3 fault conditions, LOs may miss their deadlines, but the HIs
will be delivered by their deadlines as intended properly.

--------------------------------------------------------------------------------

TXFlowP - are they injecting too frequently?

Crashes when the buffers overflow at 10ms cycles when the recipient is
off or unable to ACK? Is the queue overflowing - suspect yes

Check discarding of the HIs in HI crit mode
Finish the return to LO crit mode

Need to be ready to get the faults injected - communication with the
fault injector over USB? Or merely change the fault injector source for now
(it is OK to have a constant fault length)

Log the confirmed received packets in the listener code - seperate file for them?

Node 3 is in the last slot... sometimes it is in the last slot and
gets the sink's synchronization packet first?

Release jitter is removed? Is it removed? (check in some way)
Put the actual sending in a task to give a guard band?

--------------------------------------------------------------------------------

Node 1 - is changing crit? This is leading to the deadline miss. But
it only has LO packets... is it discarding properly?

Questions for Alan - do we need two consequtive failed ACKs to change
crit? otherwise 1 failure, then another random failure will lead to
discards of the LO?

Are the supply functions set up using a table of size 6 instead of size 7?

Check the injection timing for node 1 - which slots are receving

How long does it have to run for? (hyperperiod seems very long)
- around 200000

--------------------------------------------------------------------------------

Record the response time of the last successful ACK - attach it to the next outgoing packet
Can we get a hires timer of some kind to improve time tracking accuracy
Revert the application to its original definition (get rid of 12)

Randomise the injection timer for each flow (ensure they are not
bunching up - this could avoid any synchronisation issues)


Check the number of retransmits allocated to the LOs
MCC check to go high - check for off by one here - check buffer flushing
Reprogram all the nodes - check flow 4?

--------------------------------------------------------------------------------
Final changes for publication by both Leandro and Alan

Clarify the protocol and TinyOS components that were actually used -
check the listing of protocol components and where they came from.

Lookup the citation of tinyos-contrib repository, and any other code
components that were selected

Add the basic protocol rules into the implementation section - as
bullet points - done but got to change rule M4

Change the result logging to use the slot delays - not raw timers - done for the new results

Monitor the node failed ack counter throughout the protocol run - it
should not exceed 4 in a valid experiment

Baseline protocol should not throw away LO at all, even if it leads to
very large latencies due to blocking of the other flows

Graphs for fault behaviour - show no deadlines missed even under faults
In the baseline case, show some HI deadlines missed
Find enough faults - minimum fault seperation is 100 slots, 15 slots

First thing to check: does a 15 slot fault injection avoid deadline misses?

After changing the behaviour of the baseline to keep retransmitting,
if the baseline is increasing to unbounded latency under faults, then
increase the slot spacing from 105 to a higher value

If the node failed ACK counter is reaching 4, we are really outside
the bounds of the model. Is there anything in the application that
occurs in this case.

The paper is consistent now, Email Alan to confirm any changes that I
make - and the final graphs regenerated to remove the caption on Y axis